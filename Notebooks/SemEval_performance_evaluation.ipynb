{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation for Different Sentiment Analysis systems\n",
    "\n",
    "This notebook documents the results of evaluating different sentiment analysis systems using the datasets from SemEval 2017 Task 4-A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_gold_file(input_file):\n",
    "    '''\n",
    "    Line structure is \"sentiment \\t tweet\"\n",
    "    '''\n",
    "    senti_dict = {\n",
    "        'negative':0,\n",
    "        'neutral':1,\n",
    "        'positive':2,\n",
    "    }\n",
    "    \n",
    "    file_path = os.path.expanduser(input_file)\n",
    "    sentiments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = re.split(r'\\t',line)\n",
    "            sentiments.append(senti_dict[tokens[0]])\n",
    "    gold = np.array(sentiments)\n",
    "    \n",
    "    return gold\n",
    "\n",
    "def load_probabilities_results(input_file):\n",
    "    results = pd.read_csv(input_file,\n",
    "                           sep=\"\\t\",\n",
    "                           header=None,\n",
    "                           names=[\"negative\", \"neutral\", \"positive\"])\n",
    "    probabilities = results.to_numpy()\n",
    "    predictions = probabilities.argmax(axis=1)\n",
    "    return predictions, probabilities\n",
    "\n",
    "def load_predictions_results(input_file):\n",
    "    file_path = os.path.expanduser(input_file)\n",
    "    results, labels = [], []\n",
    "    with open(file_path, \"r\") as re_file:\n",
    "        for line in re_file:\n",
    "            tokens = re.split(r'\\t', line)\n",
    "            results.append(int(tokens[0]))\n",
    "            labels.append(int(tokens[1]))\n",
    "    predictions = np.array(results)\n",
    "    gold_labels = np.array(labels)\n",
    "        \n",
    "    return predictions, gold_labels\n",
    "\n",
    "def compute_semeval_metrics(gold, predictions):\n",
    "    \n",
    "    def _multilabel_recall(index, cmtx):\n",
    "        '''\n",
    "        Recall is defined as the proportion between correctly classified relevant classes and \n",
    "        all the known relevant classes.\n",
    "        recall = TP / TP + FN\n",
    "        '''\n",
    "        true_gold = cmtx.iloc[index, index]\n",
    "        all_gold = np.sum(cmtx.iloc[index,:].to_numpy())\n",
    "        return true_gold / all_gold\n",
    "    \n",
    "    def _multilabel_precision(index, cmtx):\n",
    "        '''\n",
    "        Precision is defined as the proportion between correctly classified cases and all the classified cases of\n",
    "        class.\n",
    "        recall = TP / TP + FP\n",
    "        '''\n",
    "        true_pred = cmtx.iloc[index, index]\n",
    "        false_pred = np.sum(cmtx.iloc[:,index].to_numpy())\n",
    "        return true_pred / false_pred\n",
    "    \n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(gold, predictions, labels=[0,1,2]), \n",
    "        index=['gold:negative', 'gold:neutral', 'gold:positive'], \n",
    "        columns=['pred:negative', 'pred:neutral', 'pred:positive']\n",
    "    )\n",
    "    \n",
    "    #accuracy\n",
    "    acc = accuracy_score(gold, predictions)\n",
    "    \n",
    "    #recall\n",
    "    negative_recall = _multilabel_recall(0, cmtx)\n",
    "    neutral_recall = _multilabel_recall(1, cmtx)\n",
    "    positive_recall = _multilabel_recall(2, cmtx)\n",
    "    avg_r = (negative_recall + neutral_recall + positive_recall) / 3\n",
    "    \n",
    "    #precision\n",
    "    negative_precision = _multilabel_precision(0, cmtx)\n",
    "    positive_precision = _multilabel_precision(2, cmtx)\n",
    "    \n",
    "    #f1\n",
    "    negative_f1 = (2*negative_precision*negative_recall) / (negative_precision+negative_recall)\n",
    "    positive_f1 = (2*positive_precision*positive_recall) / (positive_precision+positive_recall)\n",
    "    f1_pn = (positive_f1 + negative_f1) / 2\n",
    "    \n",
    "    \n",
    "    print('*******CONFUSION MATRIX*******')\n",
    "    print(cmtx)\n",
    "    print('*******EVALUATION METRICS********')\n",
    "    print('Average recall: ', avg_r)\n",
    "    print('F1_pn = ', f1_pn)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    \n",
    "def evaluate_test_file(test_results_file, mode='probs', gold_file='~/Datasets/semeval-2017/data/clean/test.tsv'):\n",
    "    gold = load_gold_file(gold_file)\n",
    "    print(\"Loaded {} test values.\".format(gold.shape[0]))\n",
    "    if mode == 'probs':\n",
    "        predictions, _ = load_probabilities_results(test_results_file)\n",
    "        print(\"Loaded {} predictions.\".format(predictions.shape[0]))\n",
    "    elif mode == 'preds':\n",
    "        predictions, predicted_labels = load_predictions_results(test_results_file)\n",
    "        print(\"Loaded {} predictions.\".format(predictions.shape[0]))\n",
    "        check = accuracy_score(gold, predicted_labels)\n",
    "        if check == 1:\n",
    "            print(\"The labels match.\")\n",
    "    compute_semeval_metrics(gold, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata_results_path = '/home/rafael/Datasets/semeval/results/fulldata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER SemEval-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2182           766           1024\n",
      "gold:neutral            1271          2663           2003\n",
      "gold:positive            150           554           1671\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.5671558001656213\n",
      "F1_pn =  0.5243033354657394\n",
      "Accuracy:  0.5304461087593618\n"
     ]
    }
   ],
   "source": [
    "vader_results = fulldata_results_path + 'vader_results2017.tsv'\n",
    "\n",
    "evaluate_test_file(vader_results, mode='preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2045          1037            890\n",
      "gold:neutral            1111          3005           1821\n",
      "gold:positive            134           608           1633\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.5695269371170385\n",
      "F1_pn =  0.5246449835877318\n",
      "Accuracy:  0.544041028980788\n"
     ]
    }
   ],
   "source": [
    "vader_01_results = fulldata_results_path + 'vader_results_01.tsv'\n",
    "evaluate_test_file(vader_01_results, mode='preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with different BERT configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different configurations are:\n",
    "\n",
    "|Model | Init Checkpoint | Pre-train | Seq.Length | Batch Size |\n",
    "|:--- | :---: | :---: | :---: | :---: |\n",
    "|**BERT_M1** | cased_based  | No  | 128 | 32 |\n",
    "|**BERT_M2** | uncased_base | No  | 128 | 32 |\n",
    "|**BERT_M3** | uncased_base | yes | 128 | 32 |\n",
    "|**BERT_M4** | uncased_base | yes | 64  | 64 |\n",
    "\n",
    "\n",
    "The experiments were conducted on the UMinho cluster. \n",
    "\n",
    "The performance measurements were obtained with the *evaluate_test_file* function, added to the *custom_bert_predict.py* script.\n",
    "\n",
    "The prediction script output is a file with the performance measurements. For each above BERT configuration, a correspondent file was generated. These files are available in the *results-from-server* folder\n",
    "\n",
    "#### The output for each file, and each configuration is given bellow.\n",
    "\n",
    "##### BERT_M1.txt\n",
    "```\n",
    "***** Predict results *****\n",
    "Loaded 12284 predictions.\n",
    "Loaded 12284 test values.\n",
    "*******CONFUSION MATRIX*******\n",
    "               pred:negative  pred:neutral  pred:positive\n",
    "gold:negative           2878           928            166\n",
    "gold:neutral            1276          3692            969\n",
    "gold:positive             90           548           1737\n",
    "*******EVALUATION METRICS********\n",
    "Average recall:  0.692601106266065\n",
    "F1_pn =  0.6813384251287284\n",
    "Accuracy:  0.6762455226310649\n",
    "```\n",
    "##### BERT_M2.txt\n",
    "```\n",
    "***** Predict results *****\n",
    "Loaded 12284 test values.\n",
    "Loaded 12284 predictions.\n",
    "The labels match.\n",
    "*******CONFUSION MATRIX*******\n",
    "               pred:negative  pred:neutral  pred:positive\n",
    "gold:negative           2979           870            123\n",
    "gold:neutral            1271          3898            768\n",
    "gold:positive             63           621           1691\n",
    "*******EVALUATION METRICS********\n",
    "Average recall:  0.7061868508225254\n",
    "F1_pn =  0.7006992300349085\n",
    "Accuracy:  0.6974926733962878\n",
    "```\n",
    "##### BERT_M3.txt\n",
    "```\n",
    "***** Predict results *****\n",
    "Loaded 12284 test values.\n",
    "Loaded 12284 predictions.\n",
    "The labels match.\n",
    "*******CONFUSION MATRIX*******\n",
    "               pred:negative  pred:neutral  pred:positive\n",
    "gold:negative           2978           867            127\n",
    "gold:neutral            1254          3841            842\n",
    "gold:positive             56           570           1749\n",
    "*******EVALUATION METRICS********\n",
    "Average recall:  0.7110430114245548\n",
    "F1_pn =  0.7039452146491718\n",
    "Accuracy:  0.6974926733962878\n",
    "```\n",
    "\n",
    "##### BERT_M4.txt\n",
    "```\n",
    "***** Predict results *****\n",
    "Loaded 12284 test values.\n",
    "Loaded 12284 predictions.\n",
    "The labels match.\n",
    "*******CONFUSION MATRIX*******\n",
    "               pred:negative  pred:neutral  pred:positive\n",
    "gold:negative           2845           936            191\n",
    "gold:neutral            1128          3741           1068\n",
    "gold:positive             44           460           1871\n",
    "*******EVALUATION METRICS********\n",
    "Average recall:  0.7113898469753331\n",
    "F1_pn =  0.6959875005243927\n",
    "Accuracy:  0.6884565288179746\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
