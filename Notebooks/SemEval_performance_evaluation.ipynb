{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_gold_file(input_file):\n",
    "    '''\n",
    "    Line structure is \"sentiment \\t tweet\"\n",
    "    '''\n",
    "    senti_dict = {\n",
    "        'negative':0,\n",
    "        'neutral':1,\n",
    "        'positive':2,\n",
    "    }\n",
    "    \n",
    "    file_path = os.path.expanduser(input_file)\n",
    "    sentiments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = re.split(r'\\t',line)\n",
    "            sentiments.append(senti_dict[tokens[0]])\n",
    "    gold = np.array(sentiments)\n",
    "    \n",
    "    return gold\n",
    "\n",
    "def load_probabilities_results(input_file):\n",
    "    results = pd.read_csv(input_file,\n",
    "                           sep=\"\\t\",\n",
    "                           header=None,\n",
    "                           names=[\"negative\", \"neutral\", \"positive\"])\n",
    "    probabilities = results.to_numpy()\n",
    "    predictions = probabilities.argmax(axis=1)\n",
    "    return predictions, probabilities\n",
    "\n",
    "def load_predictions_results(input_file):\n",
    "    file_path = os.path.expanduser(input_file)\n",
    "    results, labels = [], []\n",
    "    with open(file_path, \"r\") as re_file:\n",
    "        for line in re_file:\n",
    "            tokens = re.split(r'\\t', line)\n",
    "            results.append(int(tokens[0]))\n",
    "            labels.append(int(tokens[1]))\n",
    "    predictions = np.array(results)\n",
    "    gold_labels = np.array(labels)\n",
    "        \n",
    "    return predictions, gold_labels\n",
    "\n",
    "def compute_semeval_metrics(gold, predictions):\n",
    "    \n",
    "    def _multilabel_recall(index, cmtx):\n",
    "        '''\n",
    "        Recall is defined as the proportion between correctly classified relevant classes and \n",
    "        all the known relevant classes.\n",
    "        recall = TP / TP + FN\n",
    "        '''\n",
    "        true_gold = cmtx.iloc[index, index]\n",
    "        all_gold = np.sum(cmtx.iloc[index,:].to_numpy())\n",
    "        return true_gold / all_gold\n",
    "    \n",
    "    def _multilabel_precision(index, cmtx):\n",
    "        '''\n",
    "        Precision is defined as the proportion between correctly classified cases and all the classified cases of\n",
    "        class.\n",
    "        recall = TP / TP + FP\n",
    "        '''\n",
    "        true_pred = cmtx.iloc[index, index]\n",
    "        false_pred = np.sum(cmtx.iloc[:,index].to_numpy())\n",
    "        return true_pred / false_pred\n",
    "    \n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(gold, predictions, labels=[0,1,2]), \n",
    "        index=['gold:negative', 'gold:neutral', 'gold:positive'], \n",
    "        columns=['pred:negative', 'pred:neutral', 'pred:positive']\n",
    "    )\n",
    "    \n",
    "    #accuracy\n",
    "    acc = accuracy_score(gold, predictions)\n",
    "    \n",
    "    #recall\n",
    "    negative_recall = _multilabel_recall(0, cmtx)\n",
    "    neutral_recall = _multilabel_recall(1, cmtx)\n",
    "    positive_recall = _multilabel_recall(2, cmtx)\n",
    "    avg_r = (negative_recall + neutral_recall + positive_recall) / 3\n",
    "    \n",
    "    #precision\n",
    "    negative_precision = _multilabel_precision(0, cmtx)\n",
    "    positive_precision = _multilabel_precision(2, cmtx)\n",
    "    \n",
    "    #f1\n",
    "    negative_f1 = (2*negative_precision*negative_recall) / (negative_precision+negative_recall)\n",
    "    positive_f1 = (2*positive_precision*positive_recall) / (positive_precision+positive_recall)\n",
    "    f1_pn = (positive_f1 + negative_f1) / 2\n",
    "    \n",
    "    \n",
    "    print('*******CONFUSION MATRIX*******')\n",
    "    print(cmtx)\n",
    "    print('*******EVALUATION METRICS********')\n",
    "    print('Average recall: ', avg_r)\n",
    "    print('F1_pn = ', f1_pn)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    \n",
    "def evaluate_test_file(test_results_file, mode='probs', gold_file='~/Datasets/semeval-2017/data/clean/test.tsv'):\n",
    "    gold = load_gold_file(gold_file)\n",
    "    print(\"Loaded {} test values.\".format(gold.shape[0]))\n",
    "    if mode == 'probs':\n",
    "        predictions, _ = load_probabilities_results(test_results_file)\n",
    "        print(\"Loaded {} predictions.\".format(predictions.shape[0]))\n",
    "    elif mode == 'preds':\n",
    "        predictions, predicted_labels = load_predictions_results(test_results_file)\n",
    "        print(\"Loaded {} predictions.\".format(predictions.shape[0]))\n",
    "        check = accuracy_score(gold, predicted_labels)\n",
    "        if check == 1:\n",
    "            print(\"The labels match.\")\n",
    "    compute_semeval_metrics(gold, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata_results_path = '/home/rafael/Datasets/semeval/results/fulldata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the runs were done using bert-base arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncased bert FT with full test data\n",
    "\n",
    "Max seq: 128\n",
    "\n",
    "Batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           3030           806            136\n",
      "gold:neutral            1315          3705            917\n",
      "gold:positive             63           542           1770\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.7107185296142169\n",
      "F1_pn =  0.7020907619141197\n",
      "Accuracy:  0.6923640507977857\n"
     ]
    }
   ],
   "source": [
    "test_results = fulldata_results_path + 'bert_uncased.tsv'\n",
    "evaluate_test_file(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cased bert FT full text data and text cleaning\n",
    "\n",
    "Max seq: 128\n",
    "\n",
    "Batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 predictions.\n",
      "Loaded 12284 test values.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2878           928            166\n",
      "gold:neutral            1276          3692            969\n",
      "gold:positive             90           548           1737\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.692601106266065\n",
      "F1_pn =  0.6813384251287284\n",
      "Accuracy:  0.6762455226310649\n"
     ]
    }
   ],
   "source": [
    "cased_fulldata_clean = fulldata_results_path + 'bert_cased_clean.tsv'\n",
    "evaluate_test_file(cased_fulldata_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncased bert FT on full clean data\n",
    "\n",
    "Max seq: 128\n",
    "\n",
    "Batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 predictions.\n",
      "Loaded 12284 test values.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2529          1102            341\n",
      "gold:neutral            1129          3572           1236\n",
      "gold:positive             88           625           1662\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.6460490292146263\n",
      "F1_pn =  0.6237211639025192\n",
      "Accuracy:  0.6319602735265386\n"
     ]
    }
   ],
   "source": [
    "uncased_fulldata_clean = fulldata_results_path + 'bert_uncased_clean.tsv'\n",
    "evaluate_test_file(uncased_fulldata_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run2\n",
    "\n",
    "Max seq: 64\n",
    "\n",
    "Batch size: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 predictions.\n",
      "Loaded 12284 test values.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2504          1141            327\n",
      "gold:neutral            1099          3788           1050\n",
      "gold:positive             80           675           1620\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.6501836099418089\n",
      "F1_pn =  0.6286701298019146\n",
      "Accuracy:  0.6440898730055357\n"
     ]
    }
   ],
   "source": [
    "uncased_full_data_clean_run2 = fulldata_results_path + 'bert_uncased_clean_run2.tsv'\n",
    "evaluate_test_file(uncased_full_data_clean_run2, mode='preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncased bert in-task pre-training and FT on full clean data\n",
    "Max seq= 128\n",
    "\n",
    "Batch size:32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           3009           809            154\n",
      "gold:neutral            1312          3750            875\n",
      "gold:positive             65           588           1722\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.704745879704245\n",
      "F1_pn =  0.6959488093160748\n",
      "Accuracy:  0.6904102898078802\n"
     ]
    }
   ],
   "source": [
    "bert_itpt_ftfull = fulldata_results_path + 'bert_itpt_uncased_clean.tsv'\n",
    "evaluate_test_file(bert_itpt_ftfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 predictions.\n",
      "Loaded 12284 test values.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2568          1101            303\n",
      "gold:neutral            1152          3776           1009\n",
      "gold:positive             95           710           1570\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.6478632549777826\n",
      "F1_pn =  0.6284298230573535\n",
      "Accuracy:  0.6442526864213611\n"
     ]
    }
   ],
   "source": [
    "bert_itpt_ftfull_run2 = fulldata_results_path + 'bert_itpt_uncased_clean_run2.tsv'\n",
    "evaluate_test_file(bert_itpt_ftfull_run2, mode='preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER SemEval-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2182           766           1024\n",
      "gold:neutral            1271          2663           2003\n",
      "gold:positive            150           554           1671\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.5671558001656213\n",
      "F1_pn =  0.5243033354657394\n",
      "Accuracy:  0.5304461087593618\n"
     ]
    }
   ],
   "source": [
    "vader_results = fulldata_results_path + 'vader_results2017.tsv'\n",
    "\n",
    "evaluate_test_file(vader_results, mode='preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12284 test values.\n",
      "Loaded 12284 predictions.\n",
      "The labels match.\n",
      "*******CONFUSION MATRIX*******\n",
      "               pred:negative  pred:neutral  pred:positive\n",
      "gold:negative           2045          1037            890\n",
      "gold:neutral            1111          3005           1821\n",
      "gold:positive            134           608           1633\n",
      "*******EVALUATION METRICS********\n",
      "Average recall:  0.5695269371170385\n",
      "F1_pn =  0.5246449835877318\n",
      "Accuracy:  0.544041028980788\n"
     ]
    }
   ],
   "source": [
    "vader_01_results = fulldata_results_path + 'vader_results_01.tsv'\n",
    "evaluate_test_file(vader_01_results, mode='preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
